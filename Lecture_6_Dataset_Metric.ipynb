{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture_6_Dataset_Metric.ipynb","provenance":[],"collapsed_sections":["XIR3IFmyLd-4","WNtZVM9NKnxK"],"toc_visible":true,"authorship_tag":"ABX9TyO0CORPWOuzO3nHRXzRBeaQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Qd8yDNcCKAhm"},"source":["# Outline\n","\n","- Dataset\n"," \n","\n","- Metric\n"]},{"cell_type":"markdown","metadata":{"id":"XIR3IFmyLd-4"},"source":["### Mount Folder"]},{"cell_type":"code","metadata":{"id":"U1OvsV81LcbE","executionInfo":{"status":"ok","timestamp":1605487807057,"user_tz":-540,"elapsed":21927,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}},"outputId":"a0888e2e-8146-444c-a549-c5fccf7419a6","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WNtZVM9NKnxK"},"source":["### Loading & Checking Example Data Based on Numpy & Pandas\n"]},{"cell_type":"code","metadata":{"id":"BV4WqciSJ_gA","executionInfo":{"status":"ok","timestamp":1605487815365,"user_tz":-540,"elapsed":594,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}}},"source":["# Setting up the required libraries\n","import numpy  as np\n","import pandas as pd"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbH3vt7iLGTC","executionInfo":{"status":"ok","timestamp":1605487818041,"user_tz":-540,"elapsed":593,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}}},"source":["# The function for loading data\n","def load_LETOR4(file, num_features=46):\n","\t'''\n","\t:param file: the input file\n","\t:param num_features: the number of features\n","\t:return: the list of tuples, each tuple consists of qid, doc_reprs, doc_labels\n","\t'''\n","  \n","\tfeature_cols = [str(f_index) for f_index in range(1, num_features + 1)]\n","\n","\tdf = pd.read_csv(file, sep=\" \", header=None)\n","\tdf.drop(columns=df.columns[[-2, -3, -5, -6, -8, -9]], axis=1, inplace=True)  # remove redundant keys\n","\tassert num_features == len(df.columns) - 5\n","\n","\tfor c in range(1, num_features +2):           \t\t\t\t\t\t\t # remove keys per column from key:value\n","\t\tdf.iloc[:, c] = df.iloc[:, c].apply(lambda x: x.split(\":\")[1])\n","\n","\tdf.columns = ['rele_truth', 'qid'] + feature_cols + ['#docid', 'inc', 'prob']\n","\n","\tfor c in ['rele_truth'] + feature_cols:\n","\t\tdf[c] = df[c].astype(np.float32)\n","\n","\tdf['rele_binary'] = (df['rele_truth'] > 0).astype(np.float32)  # additional binarized column for later filtering\n","\n","\tlist_Qs = []\n","\tqids = df.qid.unique()\n","\tnp.random.shuffle(qids)\n","\tfor qid in qids:\n","\t\tsorted_qdf = df[df.qid == qid].sort_values('rele_truth', ascending=False)\n","\n","\t\tdoc_reprs = sorted_qdf[feature_cols].values\n","\t\tdoc_labels = sorted_qdf['rele_truth'].values\n","\n","\t\tlist_Qs.append((qid, doc_reprs, doc_labels))\n","\n","\t#if buffer: pickle_save(list_Qs, file=perquery_file)\n","\n","\treturn list_Qs"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"puPTp0PsL0kw"},"source":["#### Check the data"]},{"cell_type":"code","metadata":{"id":"rFgjKo50LIeJ","executionInfo":{"status":"ok","timestamp":1605487825360,"user_tz":-540,"elapsed":3700,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}},"outputId":"cdc1ff86-1e49-4000-c062-2f947484b81e","colab":{"base_uri":"https://localhost:8080/"}},"source":["file = '/content/drive/My Drive/Teaching/2020/KLIS-MLIR-2020/Data/vali_as_train.txt'\n","\n","list_Qs = load_LETOR4(file=file)\n","print('The total number of queries:', len(list_Qs))\n","\n","for (qid, doc_reprs, doc_labels) in list_Qs:\n","  print('qid:{}\\t{}\\t{}'.format(qid, doc_reprs.shape, doc_labels.shape))\n","  print('doc_reprs\\n', doc_reprs)\n","  print('doc_labels\\n', doc_labels)\n","  break"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The total number of queries: 339\n","qid:7155\t(40, 46)\t(40,)\n","doc_reprs\n"," [[0.293605 0.       0.       ... 1.       0.272727 0.      ]\n"," [0.020349 0.       0.       ... 0.4      0.318182 0.      ]\n"," [0.438953 0.       0.       ... 0.6      0.136364 0.      ]\n"," ...\n"," [0.334302 0.       0.       ... 0.8      0.242424 0.      ]\n"," [0.008721 0.       0.       ... 0.4      0.318182 0.      ]\n"," [0.017442 0.       1.       ... 0.       0.060606 0.      ]]\n","doc_labels\n"," [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BRuBX24yKtUH"},"source":["### Loading & Checking Example Data Based on PTRanking\n"]},{"cell_type":"code","metadata":{"id":"dQEUw53NhyZv","executionInfo":{"status":"ok","timestamp":1605488437038,"user_tz":-540,"elapsed":5008,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}},"outputId":"d37f5634-d787-493d-d2c9-4a276a7b90fa","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install ptranking"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting ptranking\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/10/1f39cedc6b26bb4fc9588f700f2a18bcec8a08f5f122ad1d47d54fc57324/ptranking-0.0.3-py3-none-any.whl (115kB)\n","\r\u001b[K     |██▉                             | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 71kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 6.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ptranking) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptranking) (1.18.5)\n","Installing collected packages: ptranking\n","Successfully installed ptranking-0.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZswaYz9pkX6B","executionInfo":{"status":"ok","timestamp":1605490213245,"user_tz":-540,"elapsed":1895,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}},"outputId":"60b816f1-2630-417e-ddf7-5cf06a070719","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","from ptranking.data.data_utils import LTRDataset, SPLIT_TYPE\n","\n","data_id = 'MQ2007_Super'\n","file_train = '/content/drive/My Drive/Teaching/2020/KLIS-MLIR-2020/Data/vali_as_train.txt'\n","\n","train_dataset = LTRDataset(SPLIT_TYPE.Train, data_id=data_id, file=file_train, buffer=False)\n","\n","num_train_queries = train_dataset.__len__()\n","\n","print('Dataset:\\t', data_id)\n","print('Total train queries:\\t', num_train_queries)\n","\n","for qid, torch_batch_rankings, torch_batch_std_labels in train_dataset:\n","    print('torch_batch_std_labels', torch_batch_std_labels)\n","    \n","    doc_num = torch_batch_std_labels.size(1)\n","\n","    print('torch_batch_rankings', torch_batch_rankings)\n","    break\n","    "],"execution_count":14,"outputs":[{"output_type":"stream","text":["Dataset:\t MQ2007_Super\n","Total train queries:\t 339\n","torch_batch_std_labels tensor([[1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 2., 2.,\n","         2., 2., 2., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n","         0., 1., 1., 1.]])\n","torch_batch_rankings tensor([[[0.0333, 1.0000, 0.3333,  ..., 0.1250, 0.0000, 0.0000],\n","         [0.1059, 0.1000, 0.3333,  ..., 0.1250, 0.0108, 0.0000],\n","         [0.0231, 0.2000, 0.3333,  ..., 0.1250, 0.0215, 0.0000],\n","         ...,\n","         [0.1409, 0.0000, 0.0000,  ..., 0.2500, 0.1935, 0.0000],\n","         [1.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0215, 0.0000],\n","         [0.2878, 0.0000, 0.0000,  ..., 0.3750, 0.0430, 0.0000]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rWoulL2NMnet"},"source":["## nDCG\n","\n","nDCG is a widely used metric for evaluating the performance of a ranking model. The assumptions under nDCG are that:\n","- Highly relevant documents are more useful than marginally relevant document\n","- The lower the ranked position of a relevant document, the less useful it is for the user, since it is less likely to be examined\n","\n","nDCG consists of different components, and is defined as follows:\n","The discounted cumulative gain (DCG) of a ranked list is given as:\n","$$DCG=\\sum_{i=1}^{m}\\frac{2^{y_{i}^{*}}-1}{\\log(i+1)}$$\n","where $2^{y_{i}^{*}}-1$ is usually referred to as the gain value of the $i$-th document. \n","\n","We denote the maximum DCG value attained by the ideal ranking as $DCG^{*}$, then normalizing DCG with $DCG^{*}$ gives nDCG as follows:\n","$$nDCG=\\frac{DCG}{DCG^{*}}=\\frac{1}{DCG^{*}}\\sum_{i=1}^{m}\\frac{2^{y_{i}^{*}}-1}{\\log(i+1)}$$\n","\n","### Example evaluation\n","We denote the standard labels are $0$, $1$ and $2$. In particular, $0$ means non-relevant, $1$ means marginally relevant, $2$ means very relevant. In other words, the label reveals the degree how a document is relevant to the query.\n","\n","Let's assume that there are five documents $D_1$, $D_2$, $D_3$, $D_4$ and $D_5$. The labels are given as: $D_1: 0$, $D_2: 2$, $D_3: 1$, $D_4: 0$ and $D_5: 1$.\n","\n","Furthermore, we assume that there are two candidate functions (i.e., $f_1$ and $f_2$) to be compared. The relevance predictions by $f_1$ are: $D_1: 0.3$, $D_2: 0.4$, $D_3: 0.2$, $D_4: 0.5$ and $D_5: 1.1$. The relevance predictions by $f_2$ are: $D_1: 0.1$, $D_2: 1.5$, $D_3: 0.2$, $D_4: 0.4$ and $D_5: 0.6$. \n","\n","The nDCG scores for $f_1$ and $f_2$ are computed as follows:\n","\n","First, we compute $DCG^{*}$ as:\n","$$DCG^{*}=(\\frac{2^{2}-1}{\\log_{2}(1+1)}+\\frac{2^{1}-1}{\\log_{2}(2+1)}+\\frac{2^{1}-1}{\\log_{2}(3+1)}+\\frac{2^{0}-1}{\\log_{2}(4+1)}+\\frac{2^{0}-1}{\\log_{2}(5+1)})=4.1309$$\n","\n","Second, we sort the documents according to the predictions by $f_1$ and $f_2$, respectively. For $f_1$, we get the predicted ranking as: $D_5$, $D_4$, $D_2$, $D_1$ and $D_3$. The corresponding labels are: $[1, 0, 2, 0, 1]$.\n","\n","For $f_2$, we get the predicted ranking as: $D_2$, $D_5$, $D_4$, $D_3$ and $D_1$. The corresponding labels are: $[2, 1, 0, 1, 0]$.\n","\n","Third, for $f_1$, its nDCG score is computed as:\n","$$\n","DCG_{f-1}=(\\frac{2^{1}-1}{\\log_{2}(1+1)}+\\frac{2^{0}-1}{\\log_{2}(2+1)}+\\frac{2^{2}-1}{\\log_{2}(3+1)}+\\frac{2^{0}-1}{\\log_{2}(4+1)}+\\frac{2^{1}-1}{\\log_{2}(5+1)})=2.8868$$\n","\n","Thire, for $f_2$, its nDCG score is computed as:\n","$$\n","DCG_{f-2}=(\\frac{2^{2}-1}{\\log_{2}(1+1)}+\\frac{2^{1}-1}{\\log_{2}(2+1)}+\\frac{2^{0}-1}{\\log_{2}(3+1)}+\\frac{2^{1}-1}{\\log_{2}(4+1)}+\\frac{2^{0}-1}{\\log_{2}(5+1)})=4.0616$$\n"]},{"cell_type":"markdown","metadata":{"id":"dXOSV_LWNAeB"},"source":["### Example program for computing nDCG Based on Numpy"]},{"cell_type":"code","metadata":{"id":"AfJdBi3tOQp3"},"source":["def ndcg_at_k(sys_sorted_labels, ideal_sorted_labels, k):\n","\tsys_discounted_cumu_gain_at_k = discounted_cumu_gain_at_k(sys_sorted_labels, cutoff=k)\n","\tideal_discounted_cumu_gain_at_k = discounted_cumu_gain_at_k(ideal_sorted_labels, cutoff=k)\n","\tndcg_at_k = sys_discounted_cumu_gain_at_k / ideal_discounted_cumu_gain_at_k\n","\treturn ndcg_at_k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuxBMp83M-vZ"},"source":["def discounted_cumu_gain_at_k(sorted_labels, cutoff):\n","\t'''\n","\t:param sorted_labels: ranked labels (either standard or predicted by a system) in the form of np array\n","\t:param max_cutoff: the maximum rank position to be considered\n","\t:param multi_lavel_rele: either the case of multi-level relevance or the case of listwise int-value, e.g., MQ2007-list\n","\t:return: cumulative gains for each rank position\n","\t'''\n","\tnums = np.power(2.0, sorted_labels[0:cutoff]) - 1.0\n","\n","\tdenoms = np.log2(np.arange(cutoff) + 2.0)  # discounting factor\n","\tdited_cumu_gain = np.sum(nums / denoms)\n","\n","\treturn dited_cumu_gain"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zdpk_q4zOHrV"},"source":["Based on the above example, we compute the nDCG score for $f_1$ again.\n","\n","The ideal ranking list is: $[2, 1, 1, 0, 0]$, The predicted ranking by $f_1$ is: $[1, 0, 2, 0, 1]$, thus we have"]},{"cell_type":"code","metadata":{"id":"g-HC300OOqxk","executionInfo":{"status":"ok","timestamp":1605080167786,"user_tz":-540,"elapsed":782,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}},"outputId":"4693591a-4081-46dc-f77a-5a28553f77a4","colab":{"base_uri":"https://localhost:8080/"}},"source":["sys_sorted_labels = [1, 0, 2, 0, 1] # the ranking predicted by a ranking model\n","ideal_sorted_labels=[2, 1, 1, 0, 0] # the ranking obtained according to the standard labels\n","nDCG = ndcg_at_k(sys_sorted_labels=sys_sorted_labels, ideal_sorted_labels=ideal_sorted_labels, k=5)\n","print(nDCG)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6988385132278441\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"chddnDapOc8C"},"source":["### Example program for computing nDCG Based on PTRanking"]},{"cell_type":"code","metadata":{"id":"wSlfIfqMxt6V","executionInfo":{"status":"ok","timestamp":1605492250220,"user_tz":-540,"elapsed":557,"user":{"displayName":"Hai-Tao Yu","photoUrl":"","userId":"06698935547344579595"}},"outputId":"dc22a20d-97a6-4da3-cc3a-7bf35b6dae91","colab":{"base_uri":"https://localhost:8080/"}},"source":["from ptranking.metric.adhoc_metric import torch_nDCG_at_k, torch_nDCG_at_ks\n","\n","sys_sorted_labels = torch.Tensor([1, 0, 2, 0, 1])# the ranking predicted by a ranking model\n","ideal_sorted_labels = torch.Tensor([2, 1, 1, 0, 0])# the ranking obtained according to the standard labels\n","\n","ndcg_at_k = torch_nDCG_at_k(sys_sorted_labels.view(1, -1), ideal_sorted_labels.view(1, -1), k=5)\n","print(ndcg_at_k.size(), ndcg_at_k)\n","\n","print()\n","\n","ndcg_at_ks = torch_nDCG_at_ks(sys_sorted_labels.view(1, -1), ideal_sorted_labels.view(1, -1), ks=[1, 3, 5])\n","print(ndcg_at_ks.size(), ndcg_at_ks)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["torch.Size([1, 1]) tensor([[0.6988]])\n","\n","torch.Size([1, 3]) tensor([[0.3333, 0.6052, 0.6988]])\n"],"name":"stdout"}]}]}